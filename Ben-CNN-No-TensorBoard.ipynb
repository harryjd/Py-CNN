{"cells":[{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.examples.tutorials.mnist import input_data"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Extracting MNIST_data\\train-images-idx3-ubyte.gz\nExtracting MNIST_data\\train-labels-idx1-ubyte.gz\nExtracting MNIST_data\\t10k-images-idx3-ubyte.gz\nExtracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"}],"source":["mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n","\n","def Weight_Variable(shape):\n","    tfV_initial = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n","    return tfV_initial\n","\n","# 生成偏差\n","def Bias_Vairable(shape):\n","    return tf.Variable(tf.constant(0.1, shape=shape))\n","\n","#卷积层\n","def Conv2d(x, W):\n","    #ksize必须是[1, x, y, 1]\n","    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n","\n","#池化层\n","def Max_pool_2x2(x):\n","    #ksize [1, x, y, 1]\n","    max_pool_ret = tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\\\n","            strides=[1, 2, 2, 1], padding = 'SAME')\n","    return max_pool_ret\n","\n","# 记录变量\n","def variable_summaries(var):\n","    with tf.name_scope('summaries'):\n","        mean = tf.reduce_mean(var)\n","        tf.summary.scalar('mean', mean)\n","        with tf.name_scope('stddev'):\n","            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n","        tf.summary.scalar('stddev', stddev)\n","        tf.summary.scalar('max', tf.reduce_max(var))\n","        tf.summary.scalar('min', tf.reduce_min(var))\n","        tf.summary.histogram('histogram', var)\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#region #这里开始主程序 __name__ == \"__main__\"\n","x = tf.placeholder(tf.float32, [None, 784])\n","y = tf.placeholder(tf.float32, [None, 10])\n","keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","\n","#改变x的格式，转为4D的向量\n","# -1:不限制图片数量，28,28:28x28尺寸，1:灰度图，只有1个Channel\n","x_image = tf.reshape(x, [-1, 28, 28, 1])\n","\n","#region #初始化第1层卷积层的连接权矩阵和偏置\n","W_conv1 = Weight_Variable([5, 5, 1, 32])  # 5*5的卷积核, 32个核从1个平面抽取特征\n","b_conv1 = Bias_Vairable([32])  # 共享参数，每个卷积核都用1个偏置，32个卷积核，就32个偏置\n","#把2维的x_image进行卷积\n","h_conv1 = tf.nn.relu(Conv2d(x_image, W_conv1) + b_conv1)\n","#进行pooling\n","h_pool1 = Max_pool_2x2(h_conv1)\n","#endregion\n","\n","#region #初始化第2层卷积层的连接权矩阵和偏置\n","W_conv2 = Weight_Variable([5, 5, 32, 64])  # 5*5的卷积核, 64个核从32个平面抽取特征\n","b_conv2 = Bias_Vairable([64])  # 共享参数，每个卷积核都用1个偏置，64个卷积核，就64个偏置\n","#把2维的x_image进行卷积\n","h_conv2 = tf.nn.relu(Conv2d(h_pool1, W_conv2) + b_conv2)  # 变成14*14\n","#进行pooling\n","h_pool2 = Max_pool_2x2(h_conv2)  # 变成7*7\n","#endregion\n","\n","#region #定义第1层全连接层FC1\n","# 上一层有7*7的64个图，因此展开成1维长度就是7*7*64\n","# 然后FC1层有1024个神经元，所以连接权值的维度是[7*7*64, 1024]\n","W_fc1 = Weight_Variable([7*7*64, 128])\n","b_fc1 = Bias_Vairable([128])\n","#进行FC1的运算定义\n","h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n","h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  # tf.Variable可以直接相加\n","#引入神经元的输出率\n","keep_prob = tf.placeholder(tf.float32)\n","h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n","#endregion\n","\n","#region #进行FC2的运算定义。FC2有10个神经元，采用softmax，one-hot形式，对应10个分类\n","W_fc2 = Weight_Variable([128, 10])\n","b_fc2 = Bias_Vairable([10])\n","#计算输出\n","out = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n","#endregion\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"training round= 0  and accuracy= 0.79\ntraining round= 2  and accuracy= 0.97\ntraining round= 4  and accuracy= 0.97\ntraining round= 6  and accuracy= 0.97\ntraining round= 8  and accuracy= 0.98\ntraining round= 10  and accuracy= 1.0\ntraining round= 12  and accuracy= 0.98\ntraining round= 14  and accuracy= 0.99\ntraining round= 16  and accuracy= 0.99\ntraining round= 18  and accuracy= 0.99\ntraining round= 20  and accuracy= 0.98\nTest the model, Acc= 0.9896\nFinished\n"}],"source":["#交叉熵代价函数\n","loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=out))\n","train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n","#\n","correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n","#\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","log_dir = \"Logs/log-6.1\"\n","batch_size = 100\n","#计算一共有多少个批次\n","n_batch = mnist.train.num_examples // batch_size\n","# 有85%的神经元激活\n","my_keep_prob = 0.85\n","Max_Train_Count = 21 #整个数据集反复训练的次数\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    for epoch in range(Max_Train_Count):\n","        for batch in range(n_batch):\n","            batch_xs, batch_ys = mnist.train.next_batch(batch_size)            \n","            sess.run(train_step,\n","                feed_dict={x: batch_xs, y: batch_ys, keep_prob: my_keep_prob})\n","        if(epoch % 2 == 0):\n","            acc = sess.run(accuracy,\n","                feed_dict={x: batch_xs, y: batch_ys, keep_prob: my_keep_prob})\n","            print(\"training round=\", epoch, \" and accuracy=\", acc)\n","    \n","    #最后用测试集数据来验证模型的准确率. 这一步因为数据量大，容易耗费显存。\n","    acc = sess.run(accuracy,\n","            feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n","    print('Test the model, Acc=', acc)\n","print('Finished')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}